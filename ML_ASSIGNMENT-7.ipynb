{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2ea09ad",
   "metadata": {},
   "source": [
    "# 1. What is the definition of a target function? In the sense of a real-life example, express the target function. How is a target function&#39;s fitness assessed?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80906f4",
   "metadata": {},
   "source": [
    "In machine learning, the target function, also known as the objective function or the goal function, represents the relationship between the input variables (features) and the desired output variable. It defines the mapping from inputs to outputs that the machine learning model aims to learn.\n",
    "\n",
    "    To understand the concept of a target function, let's consider a real-life example. Suppose you want to build a model that predicts house prices based on various features such as the number of bedrooms, square footage, location, etc. The target function in this case would be a mathematical representation of the relationship between these input features and the corresponding house prices.\n",
    "\n",
    "    The target function could be expressed as follows:\n",
    "\n",
    "    Price = f(Bedrooms, Square Footage, Location, ...)\n",
    "\n",
    "Here, 'Price' represents the output variable (the predicted house price), and 'Bedrooms,' 'Square Footage,' 'Location,' and other features are the input variables.\n",
    "\n",
    "The fitness or accuracy of a target function is typically assessed by comparing its predictions with the actual observed values. In the context of supervised learning, where the model is trained using labeled examples, the target function's fitness is often evaluated using a loss or error metric. Common metrics include mean squared error (MSE), mean absolute error (MAE), or accuracy (for classification problems). The lower the error or the higher the accuracy, the better the fitness of the target function. The machine learning algorithm adjusts its internal parameters during training to minimize the error or maximize the accuracy, aiming to improve the fitness of the target function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178f5675",
   "metadata": {},
   "source": [
    "# 2. What are predictive models, and how do they work? What are descriptive types, and how do you use them? Examples of both types of models should be provided. Distinguish between these two forms of models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a40583",
   "metadata": {},
   "source": [
    "\n",
    "Predictive models and descriptive models are two distinct types of models used in machine learning and data analysis. Let's explore each type and understand how they work:\n",
    "\n",
    "1. Predictive Models:\n",
    "\n",
    "    Predictive models are designed to make predictions or forecasts about future outcomes based on historical data. These models learn patterns and relationships from the input variables to predict the corresponding output variable. They are commonly used for tasks such as classification and regression.\n",
    "    \n",
    " Example 1: Linear Regression\n",
    " \n",
    "    \n",
    "    Linear regression is a predictive model used for regression tasks. It aims to fit a linear relationship between the input features and the output variable. For example, predicting house prices based on features like square footage, number of bedrooms, etc.\n",
    "\n",
    "Example 2: Random Forest\n",
    "\n",
    "     Random Forest is a predictive model used for both classification and regression tasks. It is an ensemble model that combines multiple decision trees to make predictions. It can be applied to various scenarios, such as predicting customer churn or classifying spam emails.\n",
    "\n",
    "Predictive models work by training on labeled data, where the input features and corresponding output values are known. During training, the model learns the underlying patterns and relationships in the data, enabling it to make predictions on unseen or future data.\n",
    "\n",
    "2. Descriptive Models:\n",
    "\n",
    "Descriptive models, also known as descriptive analytics, focus on understanding and summarizing historical data to gain insights and uncover patterns. These models do not aim to make predictions but rather provide a descriptive analysis of the data.\n",
    "  \n",
    "Example 1: Clustering\n",
    "\n",
    "    Clustering is a descriptive model used to group similar data points together based on their characteristics. It helps identify patterns or segments within the data. For example, clustering can be used to group customers based on their purchasing behavior.\n",
    "\n",
    "Example 2: Principal Component Analysis (PCA)\n",
    "    \n",
    "    PCA is a descriptive model used for dimensionality reduction. It aims to transform high-dimensional data into a lower-dimensional representation while preserving the most important information. It helps uncover the main underlying factors or components driving the data.\n",
    "\n",
    "Descriptive models are primarily used for exploratory data analysis, summarization, visualization, and gaining insights into the data.\n",
    "\n",
    "In summary, the main distinction between predictive models and descriptive models lies in their purpose and functionality. Predictive models focus on making predictions about future outcomes based on historical data, while descriptive models aim to understand and summarize historical data to gain insights.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32845b59",
   "metadata": {},
   "source": [
    "# 3. Describe the method of assessing a classification model&#39;s efficiency in detail. Describe the various measurement parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64a3665",
   "metadata": {},
   "source": [
    "When assessing the efficiency of a classification model, several measurement parameters are commonly used to evaluate its performance. Let's discuss these parameters in detail:\n",
    "\n",
    "1. Accuracy:\n",
    "   \n",
    "       Accuracy is the most basic measurement parameter and represents the overall correctness of the model's predictions. It is calculated by dividing the number of correct predictions by the total number of predictions. However, accuracy alone may not provide a complete picture, especially in cases where the classes are imbalanced.\n",
    "\n",
    "2. Confusion Matrix:\n",
    "\n",
    "       A confusion matrix is a tabular representation that provides a more detailed analysis of the model's performance. It breaks down the predictions into four categories: true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN). From the confusion matrix, several other evaluation metrics can be derived.\n",
    "\n",
    "3. Precision:\n",
    "      \n",
    "       Precision measures the proportion of true positive predictions among the instances predicted as positive. It is calculated as TP / (TP + FP). Precision focuses on the model's ability to avoid false positive predictions and is especially relevant when the cost of false positives is high.\n",
    "\n",
    "4. Recall (Sensitivity or True Positive Rate):\n",
    "\n",
    "       Recall measures the proportion of true positive predictions among the actual positive instances. It is calculated as TP / (TP + FN). Recall emphasizes the model's ability to correctly identify positive instances and is particularly important when the cost of false negatives is high.\n",
    "\n",
    "5. F1 Score:\n",
    "      \n",
    "       The F1 score combines precision and recall into a single metric, providing a balance between the two. It is calculated as the harmonic mean of precision and recall, given by 2 * (precision * recall) / (precision + recall). The F1 score is useful when you want to consider both precision and recall simultaneously.\n",
    "\n",
    "6. Specificity (True Negative Rate):\n",
    "\n",
    "       Specificity measures the proportion of true negative predictions among the actual negative instances. It is calculated as TN / (TN + FP). Specificity is relevant when the focus is on the model's ability to correctly identify negative instances.\n",
    "\n",
    "7. Area Under the ROC Curve (AUC-ROC):\n",
    "\n",
    "       The AUC-ROC is a popular metric used to assess the performance of a binary classification model. It measures the ability of the model to distinguish between positive and negative instances across different threshold values. A higher AUC-ROC indicates better classification performance.\n",
    "\n",
    "8. Receiver Operating Characteristic (ROC) Curve:\n",
    "\n",
    "       The ROC curve is a graphical representation of the model's performance across various threshold values. It plots the true positive rate (recall) against the false positive rate (1 - specificity) for different threshold settings. The ROC curve provides insights into the trade-off between true positives and false positives.\n",
    "\n",
    "These measurement parameters help evaluate different aspects of a classification model's performance, such as overall accuracy, precision, recall, specificity, and the balance between precision and recall. It's essential to consider multiple metrics to gain a comprehensive understanding of the model's efficiency and suitability for the specific task at hand.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb481df6",
   "metadata": {},
   "source": [
    "# 4.\n",
    "i. In the sense of machine learning models, what is underfitting? What is the most common\n",
    "reason for underfitting?\n",
    "\n",
    "ii. What does it mean to overfit? When is it going to happen?\n",
    "\n",
    "iii. In the sense of model fitting, explain the bias-variance trade-off."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec21232",
   "metadata": {},
   "source": [
    "i.    Underfitting in machine learning refers to a situation where a model is too simple or lacks the capacity to capture the underlying patterns in the data. It occurs when the model's performance is poor, both on the training data and new, unseen data. The most common reason for underfitting is when the model is too constrained or has insufficient complexity to represent the relationships within the data. For example, using a linear model to fit a highly nonlinear dataset may result in underfitting.\n",
    "\n",
    "ii.   Overfitting happens when a model becomes too complex or excessively tuned to the training data, to the point where it captures noise or irrelevant patterns. In overfitting, the model performs exceptionally well on the training data but fails to generalize to new, unseen data. Overfitting can occur when the model has too many features or parameters relative to the available training data. Additionally, insufficient regularization or overly complex models can lead to overfitting.\n",
    "\n",
    "iii.   The bias-variance trade-off is a fundamental concept in model fitting. Bias refers to the error introduced by approximating a real-world problem with a simplified model. High bias models tend to oversimplify the underlying patterns and make significant assumptions, leading to underfitting. Variance, on the other hand, refers to the model's sensitivity to fluctuations in the training data. High variance models are overly complex and capture noise, leading to overfitting.\n",
    "\n",
    "The trade-off arises because reducing bias often increases variance and vice versa. A highly biased model has low flexibility and may fail to capture complex relationships, while a high variance model is overly sensitive to the training data and may fail to generalize. The goal is to find the right balance between bias and variance by selecting an appropriate model complexity and regularization techniques. This balance aims to minimize the overall error on both the training and unseen data, resulting in a model that can generalize well to new instances.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8838ff2",
   "metadata": {},
   "source": [
    "# 5. Is it possible to boost the efficiency of a learning model? If so, please clarify how."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a33612",
   "metadata": {},
   "source": [
    "Yes, it is possible to boost the efficiency of a learning model. Here are several approaches you can consider to improve the efficiency of a machine learning model:\n",
    "\n",
    "1. Feature Engineering: Feature engineering involves transforming and selecting the most relevant features from the raw data. By creating informative and discriminative features, you can enhance the model's ability to learn and make accurate predictions. This process may involve domain knowledge, data analysis, and techniques such as dimensionality reduction, feature scaling, or creating new features based on existing ones.\n",
    "\n",
    "2. Model Selection and Hyperparameter Tuning: Different machine learning algorithms and models have varying strengths and weaknesses. It's essential to choose an appropriate model for the given task. Additionally, each model has hyperparameters that control its behavior. By tuning these hyperparameters, such as learning rate, regularization strength, or number of hidden layers, you can optimize the model's performance. Techniques like grid search or randomized search can assist in finding the optimal combination of hyperparameters.\n",
    "\n",
    "3. Cross-Validation: Cross-validation is a technique to assess the model's performance by splitting the data into multiple subsets. It helps to evaluate the model's generalization ability and identify potential issues like overfitting or underfitting. Techniques like k-fold cross-validation or stratified sampling can provide more reliable estimates of the model's performance and guide the parameter tuning process.\n",
    "\n",
    "4. Regularization: Regularization techniques help prevent overfitting by introducing a penalty term to the model's objective function. Regularization methods such as L1 (Lasso) or L2 (Ridge) regularization can reduce the model's complexity and encourage simpler solutions, leading to better generalization.\n",
    "\n",
    "5. Ensemble Methods: Ensemble methods combine multiple models to make predictions. Techniques like bagging (e.g., Random Forest), boosting (e.g., AdaBoost, Gradient Boosting), or stacking can improve the model's performance by reducing variance, handling complex relationships, and capturing diverse patterns in the data.\n",
    "\n",
    "6. Data Augmentation: Data augmentation involves artificially expanding the training dataset by applying transformations or modifications to the existing data. This technique is especially useful when the available training data is limited. By creating additional training examples, the model can learn more robust representations and generalize better to unseen data.\n",
    "\n",
    "7. Regular Model Updating: Machine learning models often require regular updates to adapt to changing data distributions or patterns. As new data becomes available, retraining the model on fresh data can improve its performance and ensure its relevance over time.\n",
    "\n",
    "These are some of the approaches to boost the efficiency of a learning model. The choice of techniques depends on the specific problem, data characteristics, and the type of model being used. Experimentation, analysis, and iterative refinement are typically involved in the process of enhancing a model's efficiency.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e38deb0",
   "metadata": {},
   "source": [
    "# 6. How would you rate an unsupervised learning model&#39;s success? What are the most common success indicators for an unsupervised learning model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4ffc61",
   "metadata": {},
   "source": [
    "\n",
    "Evaluating the success of an unsupervised learning model can be more challenging compared to supervised learning because unsupervised models typically do not have explicit labels to compare predictions against. However, there are several common indicators used to assess the success of unsupervised learning models:\n",
    "\n",
    "    Clustering Performance Metrics:    If the unsupervised learning model is designed for clustering tasks, various performance metrics can be used. Examples include the silhouette score, which measures the compactness and separation of clusters, and the Calinski-Harabasz index, which evaluates the ratio of between-cluster dispersion to within-cluster dispersion.\n",
    "\n",
    "     Visualization and Interpretability:   Unsupervised learning models often aim to discover hidden patterns or structures in the data. Visualizing the results of the model can provide insights into the discovered clusters, subgroups, or relationships. Tools like scatter plots, heatmaps, or dimensionality reduction techniques (e.g., t-SNE or PCA) can aid in visual interpretation.\n",
    "\n",
    "     Reconstruction Error:    For unsupervised dimensionality reduction techniques such as autoencoders or principal component analysis (PCA), the reconstruction error can be a useful indicator of success. It measures the accuracy of reconstructing the original input from the reduced-dimensional representation. A lower reconstruction error indicates a better representation of the data.\n",
    "\n",
    "     Outlier Detection:    Unsupervised learning models can be used to identify anomalies or outliers in the data. The success of such models can be measured by their ability to accurately detect these unusual instances.\n",
    "\n",
    "    Domain Expert Evaluation:    In some cases, domain experts can assess the results of unsupervised learning models based on their knowledge and understanding of the data. They can provide qualitative feedback on the meaningfulness and usefulness of the discovered patterns or clusters.\n",
    "\n",
    "It's important to note that the evaluation of unsupervised learning models can be subjective and context-dependent. Success indicators may vary depending on the specific task, data, and goals. It's often recommended to combine multiple evaluation measures and consider domain knowledge for a comprehensive assessment of the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec30c7f",
   "metadata": {},
   "source": [
    "# 7. Is it possible to use a classification model for numerical data or a regression model for categorical data with a classification model? Explain your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26b874e",
   "metadata": {},
   "source": [
    "\n",
    "It is not recommended to use a classification model for numerical data or a regression model for categorical data directly. The reason is that classification models and regression models are designed to handle different types of data and make different types of predictions.\n",
    "\n",
    "Classification Models for Numerical Data:\n",
    "      \n",
    "      Classification models are typically used for categorical or discrete target variables. They aim to assign input instances to predefined classes or categories. If you have numerical data as the target variable, it would not be appropriate to use a classification model directly. Instead, regression models are more suitable for predicting numerical values.\n",
    "\n",
    "Regression Models for Categorical Data:\n",
    "\n",
    "    Regression models are designed to predict continuous or numerical output values based on input features. Using a regression model directly for categorical data would not provide meaningful or accurate predictions. For categorical data, classification models should be employed to assign instances to different classes or categories.\n",
    "\n",
    "However, it's worth noting that there are techniques that allow you to adapt models to handle different data types:\n",
    "\n",
    "a. Encoding Categorical Data:\n",
    "\n",
    "    Before using a classification model, categorical data can be encoded or transformed into numerical representations. Common techniques include one-hot encoding, ordinal encoding, or label encoding. This allows the classification model to handle categorical variables appropriately.\n",
    "\n",
    "b. Discretization of Numerical Data:\n",
    "\n",
    "    If you have numerical data that you want to predict using a classification model, you can discretize it by dividing it into bins or ranges and treating it as a categorical variable. This way, you can use a classification model to predict the corresponding bin or category.\n",
    "\n",
    "In summary, while it is possible to transform the data or adapt the models to handle different types, it is generally recommended to use the appropriate model type (classification for categorical data, regression for numerical data) to achieve meaningful and accurate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82e6808",
   "metadata": {},
   "source": [
    "# 8. Describe the predictive modeling method for numerical values. What distinguishes it from categorical predictive modeling?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04625532",
   "metadata": {},
   "source": [
    "\n",
    "In numerical predictive modeling, the target variable is continuous or numerical, and regression models are used to predict its values. Evaluation metrics like MSE or RMSE are used to assess the accuracy of predictions. Categorical predictive modeling, on the other hand, deals with discrete or categorical target variables, and classification models are used to assign instances to different categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bdf357",
   "metadata": {},
   "source": [
    "# 9. The following data were collected when using a classification model to predict the malignancy of a group of patients&#39; tumors:\n",
    "i. Accurate estimates – 15 cancerous, 75 benign\n",
    "\n",
    "ii. Wrong predictions – 3 cancerous, 7 benign\n",
    "\n",
    "Determine the model&#39;s error rate, Kappa value, sensitivity, precision, and F-measure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e254937",
   "metadata": {},
   "source": [
    "To determine the performance metrics of the classification model based on the given data, we can calculate the following:\n",
    "\n",
    "i. Accurate estimates:\n",
    "\n",
    "True Positive (TP): 15 (number of cancerous tumors correctly predicted)\n",
    "True Negative (TN): 75 (number of benign tumors correctly predicted)\n",
    "ii. Wrong predictions:\n",
    "\n",
    "False Positive (FP): 7 (number of benign tumors incorrectly predicted as cancerous)\n",
    "False Negative (FN): 3 (number of cancerous tumors incorrectly predicted as benign)\n",
    "Now, let's calculate the performance metrics:\n",
    "\n",
    "Error Rate:\n",
    "\n",
    "    The error rate represents the proportion of incorrect predictions in the total predictions.\n",
    "    \n",
    "    Error Rate = (FP + FN) / (TP + TN + FP + FN)\n",
    "    \n",
    "    Error Rate = (7 + 3) / (15 + 75 + 7 + 3)\n",
    "    \n",
    "    Error Rate = 0.1 or 10% (10 out of 100 predictions were incorrect)\n",
    "\n",
    "Kappa Value:\n",
    "\n",
    "    The Kappa value measures the agreement between the actual and predicted classifications, taking into account the agreement that could occur by chance.\n",
    "    \n",
    "    Kappa = (Observed Accuracy - Expected Accuracy) / (1 - Expected Accuracy)\n",
    "    \n",
    "    Expected Accuracy = (TP + FP) * (TP + FN) / (TP + TN + FP + FN)^2 + (TN + FN) * (TN + FP) / (TP + TN + FP + FN)^2\n",
    "    \n",
    "    Expected Accuracy = (15 + 7) * (15 + 3) / (15 + 75 + 7 + 3)^2 + (75 + 3) * (75 + 7) / (15 + 75 + 7 + 3)^2\n",
    "    \n",
    "    Expected Accuracy ≈ 0.745\n",
    "    \n",
    "    Observed Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "    \n",
    "    Observed Accuracy = (15 + 75) / (15 + 75 + 7 + 3)\n",
    "    \n",
    "    Observed Accuracy = 0.9\n",
    "    \n",
    "    Kappa = (0.9 - 0.745) / (1 - 0.745)\n",
    "    \n",
    "    Kappa ≈ 0.387\n",
    "\n",
    "Sensitivity (Recall):\n",
    "\n",
    "     Sensitivity measures the model's ability to correctly identify positive cases (cancerous tumors).\n",
    "     \n",
    "     Sensitivity = TP / (TP + FN)\n",
    "     \n",
    "     Sensitivity = 15 / (15 + 3)\n",
    "\n",
    "     Sensitivity ≈ 0.833 or 83.3%\n",
    "\n",
    "Precision:\n",
    "\n",
    "    Precision measures the proportion of correctly predicted positive cases (cancerous tumors) out of all predicted positive cases.\n",
    "\n",
    "    Precision = TP / (TP + FP)\n",
    "\n",
    "    Precision = 15 / (15 + 7)\n",
    "    \n",
    "    Precision ≈ 0.682 or 68.2%\n",
    "\n",
    "F-measure:\n",
    "\n",
    "    The F-measure combines precision and recall into a single metric, providing a balanced evaluation of the model's performance.\n",
    "    \n",
    "    F-measure = 2 * (Precision * Recall) / (Precision + Recall)\n",
    "\n",
    "    F-measure = 2 * (0.682 * 0.833) / (0.682 + 0.833)\n",
    "\n",
    "    F-measure ≈ 0.751 or 75.1%\n",
    "\n",
    "In summary:\n",
    "\n",
    "    Error Rate: 10%\n",
    "\n",
    "    Kappa Value: 0.387\n",
    "\n",
    "    Sensitivity (Recall): 83.3%\n",
    "    \n",
    "    Precision: 68.2%\n",
    "    \n",
    "    F-measure: 75.1%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501b4f13",
   "metadata": {},
   "source": [
    "# 10. Make quick notes on:\n",
    "\n",
    "1. The process of holding out\n",
    "\n",
    "2. Cross-validation by tenfold\n",
    "\n",
    "3. Adjusting the parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3048898a",
   "metadata": {},
   "source": [
    "1. The process of holding out:\n",
    " \n",
    "       Splitting the data into training and validation/test sets to evaluate the model's performance.\n",
    "\n",
    "2. Cross-validation by tenfold:\n",
    "\n",
    "       Dividing the data into ten subsets, training and evaluating the model ten times using different subsets each time, and averaging the performance metrics for a robust evaluation.\n",
    "\n",
    "3. Adjusting the parameters:\n",
    "\n",
    "       Tuning the hyperparameters of a model by exploring different values to optimize its performance, often done through techniques like grid search or randomized search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267aade8",
   "metadata": {},
   "source": [
    "# 11. Define the following terms:\n",
    "1. Purity vs. Silhouette width\n",
    "2. Boosting vs. Bagging\n",
    "3. The eager learner vs. the lazy learner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e0eef2",
   "metadata": {},
   "source": [
    "\n",
    "1. Purity vs. Silhouette width:\n",
    "\n",
    "     Purity:\n",
    "       \n",
    "       Purity is a measure used in clustering algorithms to evaluate the homogeneity of clusters. It calculates the proportion of instances within a cluster that belong to the majority class. Higher purity indicates that the majority of instances within a cluster belong to the same class, implying better separation between clusters.\n",
    "    \n",
    "      Silhouette width:\n",
    "      \n",
    "       Silhouette width is another measure used in clustering algorithms to assess the quality of clustering. It calculates the cohesion (how close instances are to their own cluster) and separation (how far instances are from neighboring clusters) within a cluster. A higher silhouette width indicates better-defined and well-separated clusters.\n",
    "       \n",
    "2. Boosting vs. Bagging:\n",
    "\n",
    "      Boosting: \n",
    "      \n",
    "        Boosting is an ensemble learning technique that combines multiple weak learners (usually decision trees) to create a strong learner. It trains models sequentially, with each subsequent model focusing on instances that were misclassified by previous models. Boosting aims to improve the overall performance by giving more weight to challenging instances and iteratively adjusting the model to improve predictions.\n",
    "       \n",
    "      Bagging:\n",
    "      \n",
    "       Bagging (Bootstrap Aggregating) is another ensemble learning technique that involves training multiple models (typically with the same algorithm) on different subsets of the training data, obtained through bootstrap sampling. The models are trained independently, and their predictions are combined through averaging or voting. Bagging helps to reduce variance and improve stability by reducing the impact of individual instances on the overall prediction.\n",
    "       \n",
    "3. Eager learner vs. Lazy learner:\n",
    "\n",
    "      Eager learner: \n",
    "       \n",
    "         An eager learner, also known as an eager classifier, eagerly builds a classification model during the training phase. It constructs a general representation of the training data and makes predictions based on this pre-built representation. Examples of eager learners include decision trees and artificial neural networks. Eager learners tend to have longer training times but can provide faster predictions once the model is built.\n",
    "       \n",
    "     Lazy learner: \n",
    "     \n",
    "        A lazy learner, also known as an instance-based learner, defers the construction of the classification model until a prediction is needed. Instead of building a general representation of the data, lazy learners store the training instances and use them directly during prediction. K-nearest neighbors (KNN) algorithm is a common example of a lazy learner. Lazy learners have faster training times but may have slower prediction times as they need to compare the query instance to stored instances during prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad131069",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
